{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, read in the data file created from psidtools [here](https://github.com/nilshg/psidJulia/blob/master/create_panel.do):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import statsmodels.formula.api as sm\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_stata('/Users/tew207/Desktop/data_core_1968_2013.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preliminary defintions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tinit = 1967; tlast = 2013\n",
    "years = range(1968,1998) + range(1999,2015,2)\n",
    "ageinit = 20; agelast = 64\n",
    "agecell = 4\n",
    "minyrs = 20\n",
    "nlag = 29\n",
    "agelb = 19; ageub = agelb + agecell\n",
    "agemidpt = (agelb+ageub)/2  # = 21\n",
    "agemax = (agelast + agelast - agecell)/2 - (ageub+agelb)/2  # = 41 (maximum age a cohort can reach)\n",
    "oldcoh = agelast - minyrs - ageinit - agecell/2   # number of cohort existing in the first year\n",
    "newcoh = 18  # num of cohorts entering after 1968 (hardcoded as cohorts working past 1997 only get an obs every 2 years)\n",
    "maxcoh = oldcoh + newcoh  # total number of cohorts;\n",
    "\n",
    "# Guvenen uses the person identifier as seqno1968, let's recover that from the psidtools output:\n",
    "data[\"seqno1968\"] = data[\"x11101ll\"].astype(str).str[-3:].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we have to fill in the missing hourly wages for 1993, which were not calculated in the PSID of that year (as the methodology for splitting farm and business income into labor and asset components had changed, see explanation [on page 7 here](https://psidonline.isr.umich.edu/data/Documentation/Fam/1993/93guide.pdf)). It is easy to verify from Guvenen's dataset that he simply generated this variable by dividing total labor income in 1992 by hours worked in 1992, so we will do the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"hdwg1993\"] = data.hdlbinc1993/data.hwkhrs1993\n",
    "data.loc[~np.isfinite(data[\"hdwg1993\"]), \"hdwg1993\"] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some changes to the variables containing information about educational achievement, which is tricky as there are multiple sources of information. The series to use for most years is the one starting with `ER30010` in 1968, however this variable does not exist in 1969 and does not contain information on heads from 1970 to 1975. The easiest solution is to use variables `ER30110` in 1972 and `ER30181` in 1975, and assign the most recent observation to the missing values in 1969, 1970, 1971, 1973, and 1974.\n",
    "\n",
    "An alternative approach could be to use the bracketed information on education in `V794` to `V3663` from 1969 to 1974, however this approach is not taken here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use ER30110 in 1972 and ER30181 in 1975\n",
    "data[\"grades1972\"] = data[\"extragrades1972\"] \n",
    "data[\"grades1975\"] = data[\"extragrades1975\"]\n",
    "data.loc[data[\"grades1972\"]>25, \"grades1972\"] = np.nan # take out values of grade above 25\n",
    "data.loc[data[\"grades1975\"]>25, \"grades1975\"] = np.nan\n",
    "data.drop([\"extragrades1972\", \"extragrades1975\"], axis=1, inplace=True)\n",
    "\n",
    "data[\"grades1969\"] = 0\n",
    "data.loc[(data.seqno1968==1) & (data.seqno1969==1), \"grades1969\"] = data[(data.seqno1968==1) & (data.seqno1969==1)].grades1968\n",
    "cond = (data.seqno1972==1) & (data.seqno1969==1) & (data.grades1969==0)\n",
    "data.loc[cond, \"grades1969\"] = data[cond].grades1972\n",
    "\n",
    "data.loc[(data.seqno1968==1) & (data.seqno1970==1), \"grades1970\"] = data[(data.seqno1968==1) & (data.seqno1969==1)].grades1968\n",
    "cond = (data.seqno1970==1) & (data.seqno1972==1) & (data.grades1969==0)\n",
    "data.loc[cond, \"grades1970\"] = data[cond].grades1972\n",
    "\n",
    "data.loc[(data.seqno1971==1) & (data.seqno1970==1), \"grades1971\"] = data[(data.seqno1971==1) & (data.seqno1970==1)].grades1970\n",
    "cond = (data.seqno1971==1) & (data.seqno1972==1) & (data.grades1971==0)\n",
    "data.loc[cond, \"grades1971\"] = data[cond].grades1972\n",
    "\n",
    "data.loc[(data.seqno1973==1) & (data.seqno1972==1), \"grades1973\"] = data[(data.seqno1973==1) & (data.seqno1972==1)].grades1972\n",
    "cond = (data.grades1973==0) & (data.seqno1973==1) & (data.seqno1975==1)\n",
    "data.loc[cond, \"grades1973\"] = data[cond].grades1975\n",
    "\n",
    "data.loc[(data.seqno1974==1) & (data.seqno1972==1), \"grades1974\"] = data[(data.seqno1974==1) & (data.seqno1972==1)].grades1972\n",
    "cond = (data.grades1974==0) & (data.seqno1974==1) & (data.seqno1975==1)\n",
    "data.loc[cond, \"grades1974\"] = data[cond].grades1975\n",
    "\n",
    "for i in years:\n",
    "    data.loc[data[\"grades\"+str(i)]>30, \"grades\"+str(i)] = np.nan # take out grades above 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert labour income and hourly wages into real terms by dividing by the GDP deflator obtained from [FRED](https://research.stlouisfed.org/fred2/series/GDPDEF#) (where we divide labor income in 1968 by the GDP deflator for 1967, as the PSID asks about income in the previous year):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prc = {\"1967\":0.258, \"1968\":0.2689, \"1969\":0.2822, \"1970\":0.2971, \"1971\":0.3121, \"1972\":0.3256, \n",
    "       \"1973\":0.3434, \"1974\":0.3743, \"1975\":0.4088, \"1976\":0.4313, \"1977\":0.458, \"1978\":0.4901, \n",
    "       \"1979\":0.5307, \"1980\":0.5786, \"1981\":0.6327, \"1982\":0.6719, \"1983\":0.6983, \"1984\":0.7231, \n",
    "       \"1985\":0.7463, \"1986\":0.7614, \"1987\":0.7807, \"1988\":0.8081, \"1989\":0.8395, \"1990\":0.8706, \n",
    "       \"1991\":0.8996, \"1992\":0.9201, \"1993\":0.942, \"1994\":0.962, \"1995\":0.9821, \"1996\":1.0, \n",
    "       \"1997\":1.0171, \"1998\":1.0282, \"1999\":1.0439, \"2000\":1.0676, \"2001\":1.092, \"2002\":1.1088, \n",
    "       \"2003\":1.1308, \"2004\":1.1619, \"2005\":1.1993, \"2006\":1.2362, \"2007\":1.2691, \"2008\":1.2941, \n",
    "       \"2009\":1.3039, \"2010\":1.3197, \"2011\":1.347, \"2012\":1.3712}\n",
    "\n",
    "for i in years:\n",
    "    data[\"rhdlbinc\"+str(i)] = data[\"hdlbinc\"+str(i)]/prc[str(i-1)]\n",
    "    data[\"rhdwg\"+str(i)] = data[\"hdwg\"+str(i)]/prc[str(i-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define a dictionary containing the relative real wages for years 1967 to 2012 to create a relative wage for each year.\n",
    "Replace all all observations with missing values which have either:\n",
    "\n",
    "1. Head real wage less than \\$ 2 or more than \\$ 520 in 1996 prices\n",
    "2. Head hours less than 520 or more than 5096\n",
    "3. Positive hours and no income or positive income and no hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "awg = {\"1967\":2.85, \"1968\":3.02, \"1969\":3.22, \"1970\":3.40, \"1971\":3.63, \"1972\":3.90,\n",
    "       \"1973\":4.14, \"1974\":4.43, \"1975\":4.73, \"1976\":5.06, \"1977\":5.44, \"1978\":5.87,\n",
    "       \"1979\":6.33, \"1980\":6.84, \"1981\":7.43, \"1982\":7.86, \"1983\":8.19, \"1984\":8.48,\n",
    "       \"1985\":8.73, \"1986\":8.92, \"1987\":9.13, \"1988\":9.43, \"1989\":9.80, \"1990\":10.19,\n",
    "       \"1991\":10.50, \"1992\":10.76, \"1993\":11.03, \"1994\":11.32, \"1995\":11.64, \"1996\":12.03,\n",
    "       \"1997\":12.51, \"1998\":13.01, \"1999\":13.49, \"2000\":14.01, \"2001\":14.54, \"2002\":14.96,\n",
    "       \"2003\":15.36, \"2004\":15.69, \"2005\":16.12, \"2006\":16.74, \"2007\":17.41, \"2008\":18.07, \n",
    "       \"2009\":18.61, \"2010\":19.06, \"2011\":19.44, \"2012\":19.73}\n",
    "\n",
    "for i in years:\n",
    "    data[\"rawg\"+str(i)] = awg[str(i-1)]/prc[str(i-1)] # convert to 1996 prices\n",
    "    # hourly wages that are lower than $2 or higher than $400in 1996 prices\n",
    "    cond1 = (data[\"rhdwg\"+str(i)] <= 2*data[\"rawg\"+str(i)]/awg[\"1996\"]) | (data[\"rhdwg\"+str(i)] > 400*data[\"rawg\"+str(i)]/awg[\"1996\"]) \n",
    "    # yearly working hours observations above 5096 or below 520\n",
    "    cond2 = (data[\"hwkhrs\"+str(i)] > 5096) | (data[\"hwkhrs\"+str(i)] < 520)\n",
    "    # individuals reporting positive hours but no income\n",
    "    cond3 = (data[\"rhdlbinc\"+str(i)] == 0) & (data[\"hwkhrs\"+str(i)] > 0)\n",
    "    # individuals reporting positive income, but no hours\n",
    "    cond4 = (data[\"rhdlbinc\"+str(i)] > 0) & (data[\"hwkhrs\"+str(i)] == 0)\n",
    "    # replace observations fulfilling any of the above criteria with missing\n",
    "    data.loc[np.logical_or.reduce((cond1, cond2, cond3, cond4)), \"rhdlbinc\"+str(i)] = np.nan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create dummies for:\n",
    "\n",
    "1. Age between 20 and 64 inclusive\n",
    "2. Individual is head\n",
    "2. Sex male\n",
    "3. Labour income positive\n",
    "\n",
    "and record the number of years for which all four dummies are equal to one in a new variable `kept`.\n",
    "Remove all observations with less than `minyrs` years for which all dummies are one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1685 individuals with at least 20 years of valid observations\n"
     ]
    }
   ],
   "source": [
    "data[\"kept\"] = 0\n",
    "\n",
    "for i in years:\n",
    "    # Dummy for 19<age<65\n",
    "    data[\"dum_age\"+str(i)] = 0 \n",
    "    data.loc[(data[\"agehd\"+str(i)] >= ageinit) & (data[\"agehd\"+str(i)] <= agelast), \"dum_age\"+str(i)] = 1  \n",
    "    # Dummy for head of household\n",
    "    data[\"dum_seq\"+str(i)] = 0 \n",
    "    data.loc[data[\"seqno\"+str(i)]==1, \"dum_seq\"+str(i)] = 1 \n",
    "    # Dummy for sex of head \n",
    "    data[\"dum_sex\"+str(i)] = 0 \n",
    "    data.loc[data[\"sexhd\"+str(i)]==1 ,\"dum_sex\"+str(i)] = 1  \n",
    "    # Dummy for positive labour income\n",
    "    data[\"dum_lab\"+str(i)] = 0 \n",
    "    data.loc[data[\"rhdlbinc\"+str(i)]>0, \"dum_lab\"+str(i)] = 1\n",
    "    # Dummy for agedum*headdum*sexdum*incdum \n",
    "    data.kept += data[\"dum_age\"+str(i)]*data[\"dum_seq\"+str(i)]*data[\"dum_sex\"+str(i)]*data[\"dum_lab\"+str(i)]\n",
    "    # Generate log income\n",
    "    data[\"logrinc\"+str(i)] = np.log(data[\"rhdlbinc\"+str(i)])\n",
    "    # Experience is age - education (up to 12 years) - 6 \n",
    "    data[\"edu_capped\"] = data[\"grades\"+str(i)].fillna(0)\n",
    "    data.loc[data[\"edu_capped\"]<12, \"edu_capped\"] = 12\n",
    "    data[\"expr\"+str(i)] = data[\"agehd\"+str(i)] - data[\"edu_capped\"] - 6\n",
    "    \n",
    "print \"There are\", sum(data.kept>=minyrs), \"individuals with at least\", minyrs, \"years of valid observations\"\n",
    "data = data[data.kept>=minyrs]\n",
    "\n",
    "data[\"unidno\"] = range(1,data.shape[0]+1)\n",
    "data[\"k\"] = data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop some variables and rename those variables that might create problems when identifying stub names in wide-to-long conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_drop = [\"hdgrades\", \"prc\", \"rawg\", \"awg\", \"edu_capped\", \"yrdum\", \"hdwg\", \"hdlbinc\", \"rhdlbin\", \"rhdwg\", \"hwkhrs\", \"relhd\", \"sexhd\"]\n",
    "drop_all = []\n",
    "\n",
    "for d in to_drop:\n",
    "    for colname in data.columns:\n",
    "        if colname[0:len(d)]==d:\n",
    "            drop_all.append(colname)\n",
    "            \n",
    "data.drop(drop_all, axis=1, inplace=True)\n",
    "\n",
    "for i in years:\n",
    "    data.rename(columns={\"agehd\"+str(i) : \"hdage\"+str(i),\n",
    "                         \"hdwg\"+str(i) : \"nhdwg\"+str(i), \n",
    "                         \"age\"+str(i) : \"indage\"+str(i)}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the data set from wide to long format; create squares, cubes and quadruples of experience, drop observations outside first and last year of analysis and create year dummies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 observations below year 1967 dropped\n",
      "0 observations above year 2013 dropped\n"
     ]
    }
   ],
   "source": [
    "data_long = pd.wide_to_long(data, ['hdage', 'dum_age', 'expr', 'grades', 'indage', \n",
    "                                   'id', 'dum_lab', 'logrinc', 'dum_seq', \n",
    "                                   'seqno', 'dum_sex'], i=\"unidno\", j=\"years\")\n",
    "\n",
    "data_long.reset_index(inplace=True)\n",
    "data_long[\"year\"] = data_long.years.astype(int)\n",
    "\n",
    "# create squared, cubed, quadrupled experience variables\n",
    "data_long[\"agehdsq\"] = data_long[\"expr\"]**2/100\n",
    "data_long[\"agehdcu\"] = data_long[\"expr\"]**3/1000\n",
    "data_long[\"agehdqr\"] = data_long[\"expr\"]**4/10000\n",
    "\n",
    "# drop sample outside initial/last year range\n",
    "print sum(data_long.year<=tinit), \"observations below year\", tinit, \"dropped\"\n",
    "print sum(data_long.year>tlast), \"observations above year\", tlast, \"dropped\"\n",
    "data_long = data_long[(data_long.years > tinit)&(data_long.years <= tlast)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create year dummies, run regression\n",
    "data = pd.concat([data_long, pd.get_dummies(data_long[\"year\"], prefix=\"yrdum\")], axis=1)\n",
    "result = sm.ols(formula = \"logrinc ~ hdage + agehdsq +\"+\"+\".join([\"yrdum_\"+str(i) for i in years]), data=data).fit()\n",
    "data = data[[col for col in data.columns if col[:5] != 'yrdum']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each year from 1968 to 1996, fit the regression:\n",
    "$$\n",
    "y_t = \\beta_0 + \\beta_1 age_t + \\beta_2 expr^2_t + \\beta_3 expr^3_t + \\varepsilon_t\n",
    "$$\n",
    "and record the resulting contants, coefficients and residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newcols = [\"alphaage\", \"alphagesq\", \"alphagecu\", \"alphacons\", \"residual\"]\n",
    "data = pd.concat([data, pd.DataFrame(index=data.index, columns=newcols, dtype=float)], axis=1)\n",
    "\n",
    "for i in years:\n",
    "    cond = (data[\"year\"]==i) & (data.seqno*data.dum_lab*data.dum_age*data.dum_sex==1)\n",
    "    # fit regression\n",
    "    result = sm.ols(\"logrinc ~ hdage + agehdsq + agehdcu\", data=data[cond]).fit()\n",
    "    # save coefficients\n",
    "    data.loc[cond, [\"alphaage\", \"alphagesq\", \"alphagecu\", \"alphacons\"]] = list(result.params)\n",
    "    # save residuals\n",
    "    data.loc[cond, \"residual\"] = result.resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform time-series operations to create the lag/lead residuals (important: order data by idno, year before shifting columns up to create lagged residuals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-allocate columns for speed\n",
    "data[\"combined\"] = data.seqno*data.dum_lab*data.dum_sex*data.dum_age\n",
    "newcols = [\"resid\"+str(i)+\"_\"+str(j) for i in range(1,agemax+1) for j in range(1,tlast-tinit+2)]\n",
    "data = pd.concat([data, pd.DataFrame(index=data.index, columns=newcols, dtype=float)], axis=1)\n",
    "newcols = [\"resid\"+str(i)+\"f\"+str(j) for i in range(1,agemax+1) for j in range(1,tlast-tinit+2)]\n",
    "data = pd.concat([data, pd.DataFrame(index=data.index, columns=newcols, dtype=float)], axis=1)\n",
    "\n",
    "# Sort by id and year, reset index \n",
    "data.sort(columns=[\"unidno\", \"year\"], inplace=True)\n",
    "data.index = range(1, len(data)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the lagged covariance vectors as columns in the dataframe\n",
    "j = ageub  # Cohort age upper bound\n",
    "m = 0      # Cohort age midpoint\n",
    "\n",
    "for i in range(agelb+1,agelast-agecell+1):\n",
    "    j += 1    # Increment upper age bound\n",
    "    m += 1    # Increment age mid-point\n",
    "    n = 0     # Lag counter\n",
    "    for k in years:\n",
    "        t = k - 1968 + 1  # t=1 is 1968, t=46 is 2013\n",
    "        cond = (data.combined==1) & (data.hdage.isin(range(i,j+1))) & (data.year==k)\n",
    "        data.loc[cond, \"resid\"+str(m)+\"_\"+str(t)] = data[cond].residual\n",
    "        data.loc[:, \"resid\"+str(m)+\"f\"+str(t)] = data[\"resid\"+str(m)+\"_\"+str(t)].shift(-n)\n",
    "        data.drop(\"resid\"+str(m)+\"_\"+str(t), axis=1, inplace=True)\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a `(maxcoh, agemax, agemax)` matrix that holds the autocovariances between residuals at different lags for different cohorts, and a similar sized matrix to record the observations used to calculate those covariances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cov holds the covariances, N the observations used to calculate them\n",
    "Cov = np.full((maxcoh, agemax, agemax), 0)\n",
    "N = np.full((maxcoh, agemax, agemax), 0)\n",
    "\n",
    "for time in range(1, tlast-tinit+1):\n",
    "    for age in range(1, agemax+1):\n",
    "        c = min(age,time,nlag)\n",
    "        for k in range(1,c+1):\n",
    "            m = c - k\n",
    "            l = age - m\n",
    "            x = time - m\n",
    "            cohort = age - time + newcoh + 1\n",
    "            cond = (~np.isnan(data[\"resid\"+str(age)+\"f\"+str(time)])) & (~np.isnan(data[\"resid\"+str(l)+\"f\"+str(x)]))\n",
    "            obs = len(data[cond])\n",
    "            if (obs>10) & (cohort in range(1, maxcoh+1)):\n",
    "                cov = np.cov(data.loc[cond, \"resid\"+str(age)+\"f\"+str(time)], data.loc[cond, \"resid\"+str(l)+\"f\"+str(x)])\n",
    "                Cov[cohort-1, age-1, l-1] = cov[0,1]\n",
    "                N[cohort-1, age-1, l-1] = obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, export the covariances and observations as a `hdf5` file to be read in by the optimization routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "output = h5py.File('output_'+str(tinit+1)+'_'+str(tlast)+'.h5', 'w')\n",
    "output.create_dataset('Covariances', data=Cov)\n",
    "output.create_dataset('Observations', data=N)\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
